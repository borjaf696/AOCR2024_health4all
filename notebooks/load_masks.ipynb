{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "IMAGES_TO_LOAD = 100\n",
    "\n",
    "def crop_volume(volume, min_bounds, max_bounds):\n",
    "    cropped_volume = volume[min_bounds[0]:max_bounds[0]+1,\n",
    "                            min_bounds[1]:max_bounds[1]+1,\n",
    "                            min_bounds[2]:max_bounds[2]+1]\n",
    "    return cropped_volume\n",
    "\n",
    "def reshape_input_torch(volume):\n",
    "    # Cambiar de [100, 512, 512, 1] a [T, H, W, C] usando torch.permute()\n",
    "    reshaped = volume.permute(3, 0, 1, 2)\n",
    "    reshaped = reshaped.unsqueeze(-1)\n",
    "    return reshaped\n",
    "\n",
    "def find_bounds(volume):\n",
    "    nonzero_indices = torch.nonzero(volume > 0)\n",
    "    if nonzero_indices.nelement() == 0:\n",
    "        return None, None\n",
    "    min_bounds = torch.min(nonzero_indices, dim=0).values\n",
    "    max_bounds = torch.max(nonzero_indices, dim=0).values\n",
    "    return min_bounds, max_bounds\n",
    "\n",
    "def update_global_bounds(global_min, global_max, min_bounds, max_bounds):\n",
    "    if global_min is None or global_max is None:\n",
    "        return min_bounds, max_bounds\n",
    "    new_global_min = torch.min(global_min, min_bounds)\n",
    "    new_global_max = torch.max(global_max, max_bounds)\n",
    "    return new_global_min, new_global_max\n",
    "\n",
    "def load_nii_image(folder, file_name):\n",
    "    file_path = os.path.join(folder, file_name)\n",
    "    image_nifti = nib.load(file_path)\n",
    "    return image_nifti.get_fdata()\n",
    "\n",
    "def calculate_bounds(mask_folder):\n",
    "    global_max, global_min = None, None\n",
    "    for i, file_name in enumerate(os.listdir(mask_folder)):\n",
    "        if i >= IMAGES_TO_LOAD:\n",
    "            break\n",
    "        if file_name.endswith('.nii.gz'):\n",
    "            mask_image = torch.from_numpy(\n",
    "                load_nii_image(mask_folder, file_name)\n",
    "            )\n",
    "            # Find volumes\n",
    "            min_bounds, max_bounds = find_bounds(mask_image)\n",
    "            # Calculate bounds globally\n",
    "            if min_bounds is not None and max_bounds is not None:\n",
    "                global_min, global_max = update_global_bounds(\n",
    "                    global_min, \n",
    "                    global_max, \n",
    "                    min_bounds, \n",
    "                    max_bounds\n",
    "                )\n",
    "    return global_min, global_max\n",
    "\n",
    "def repeat_permute_images(image):\n",
    "    # Let's repeat the input to adapt it to the 3 required channels\n",
    "    image = image.repeat(3, 1, 1, 1) \n",
    "    # Permutate the input to match the dimensions in the model\n",
    "    image = image.permute(0, 3, 1, 2)\n",
    "    return image\n",
    "\n",
    "def check_shapes(v1, v2):\n",
    "    if len(v1) != len(v2):\n",
    "        return False\n",
    "    for i in range(len(v1)):\n",
    "        if v1[i] != v2[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Load the images\n",
    "def preprocess_nii_images(folder:str , output_folder: str = \"../aocr2024/preprocessed_images/\", bounds: list = None, test: bool = False):\n",
    "    min_shape_image = [\n",
    "        int(bounds[1][0] - bounds[0][0]) + 1, \n",
    "        int(bounds[1][1] - bounds[0][1]) + 1, \n",
    "        int(bounds[1][2] - bounds[0][2]) + 1\n",
    "    ]\n",
    "    reductions_stored = []\n",
    "    removed_images = 0\n",
    "    for i, file_name in enumerate(os.listdir(folder)):\n",
    "        if file_name.endswith('.nii.gz'):            \n",
    "            image_array = load_nii_image(folder, file_name)\n",
    "            image_array_torch = torch.from_numpy(image_array)\n",
    "            shape_original = image_array_torch.shape\n",
    "            size_original = (\n",
    "                shape_original[0] *\n",
    "                shape_original[1] *\n",
    "                shape_original[2]\n",
    "            )\n",
    "            preprocessed_image = crop_volume(\n",
    "                image_array_torch,\n",
    "                bounds[0],\n",
    "                bounds[1]\n",
    "            )\n",
    "            if not check_shapes(preprocessed_image.shape, min_shape_image):\n",
    "                removed_images += 1\n",
    "                continue\n",
    "            preprocessed_image = repeat_permute_images(\n",
    "                preprocessed_image\n",
    "            )\n",
    "            shape_new = preprocessed_image.shape\n",
    "            size_new = (\n",
    "                shape_new[0] *\n",
    "                shape_new[1] *\n",
    "                shape_new[2]\n",
    "            )\n",
    "            \n",
    "            # Calculate ratio between original and new size\n",
    "            ratio = size_new / size_original * 100\n",
    "            reductions_stored.append(\n",
    "                ratio\n",
    "            )\n",
    "            numpy_array = preprocessed_image.cpu().numpy()\n",
    "            nifti_image = nib.Nifti1Image(numpy_array, affine=np.eye(4))\n",
    "            nib.save(nifti_image, f\"{output_folder}/{file_name}\")\n",
    "            \n",
    "    print(f\"Preprocessed and stored {i} images\")\n",
    "    print(f\"Percentage removed: {removed_images / i * 100:.2f}%\")\n",
    "    return reductions_stored\n",
    "    \n",
    "# Get the global bounds\n",
    "global_min, global_max = calculate_bounds(\"../aocr2024/2_Train,Valid_Mask/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and train or only test\n",
    "only_test = False\n",
    "# Crop the images with the new bounds\n",
    "print(f\"Thresholds - min {global_min}, max {global_max}\")\n",
    "if only_test:\n",
    "    ratios = preprocess_nii_images(\"../aocr2024/3_Test1_Image/\", bounds = [global_min,  global_max], output_folder = \"../aocr2024/preprocessed_images_test/\")\n",
    "else:\n",
    "    ratios = preprocess_nii_images(\"../aocr2024/1_Train,Valid_Image/\", bounds = [global_min,  global_max], output_folder = \"../aocr2024/preprocessed_images/\")\n",
    "    ratios = preprocess_nii_images(\"../aocr2024/3_Test1_Image/\", bounds = [global_min,  global_max], output_folder = \"../aocr2024/preprocessed_images_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
