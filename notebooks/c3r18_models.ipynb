{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "selected_model = \"mc3d18\"\n",
    "batch_size = 16\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class ModifiedR3D18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedR3D18, self).__init__()\n",
    "        self.r3d_18 = models.video.r3d_18(\n",
    "            weights=models.video.R3D_18_Weights.DEFAULT\n",
    "        )\n",
    "        num_ftrs = self.r3d_18.fc.in_features\n",
    "        self.r3d_18.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 1)         \n",
    "        )\n",
    "\n",
    "    def to_device(self, device):\n",
    "        self.r3d_18 = self.r3d_18.to(device)\n",
    "        weights = self.r3d_18.state_dict()\n",
    "        for name, param in weights.items():\n",
    "            if param.is_distributed:\n",
    "                weights[name] = param.to(device)\n",
    "        self.r3d_18.load_state_dict(weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.r3d_18(x)\n",
    "\n",
    "class ModifiedMC3_18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedMC3_18, self).__init__()\n",
    "        self.mc3_18 = models.video.mc3_18(\n",
    "            weights=models.video.MC3_18_Weights.DEFAULT\n",
    "        )\n",
    "        num_ftrs = self.mc3_18.fc.in_features\n",
    "        self.mc3_18.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 1)         \n",
    "        )\n",
    "\n",
    "    def to_device(self, device):\n",
    "        self.mc3_18 = self.mc3_18.to(device)\n",
    "        weights = self.mc3_18.state_dict()\n",
    "        for name, param in weights.items():\n",
    "            if param.is_distributed:\n",
    "                weights[name] = param.to(device)\n",
    "        self.mc3_18.load_state_dict(weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mc3_18(x)\n",
    "\n",
    "class LazyImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_file, validation_file, split = \"Train\"):\n",
    "        self.image_dir = image_dir\n",
    "        self.__type_of_data = split\n",
    "        self.image_filenames = set([filename.split(\".\")[0] for filename in os.listdir(image_dir)])\n",
    "        if self.__type_of_data != \"Test\":\n",
    "            self.__df_validation = pd.read_csv(\n",
    "                validation_file,\n",
    "            )\n",
    "            self.df_labels = pd.read_csv(\n",
    "                labels_file\n",
    "            )\n",
    "            self.image_filenames = self.image_filenames.intersection(\n",
    "                set(\n",
    "                    self.__df_validation.loc[\n",
    "                        self.__df_validation.group == self.__type_of_data,\n",
    "                        \"id\"\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        self.image_filenames = [f\"{filename}.nii.gz\" for filename in self.image_filenames]        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image_nifti = nib.load(img_name)\n",
    "        image = torch.from_numpy(\n",
    "            image_nifti.get_fdata()\n",
    "        ).float()\n",
    "        # Get label\n",
    "        id = img_name.split(\"/\")[-1].split(\".\")[0]\n",
    "        if self.__type_of_data == \"Test\":\n",
    "            label = id\n",
    "        else:\n",
    "            label = int(\n",
    "                    self.df_labels.loc[\n",
    "                        self.df_labels.id == id,\n",
    "                        \"label\"\n",
    "                    ].iloc[\n",
    "                        0\n",
    "                    ]\n",
    "                )\n",
    "        return image, label\n",
    "    \n",
    "# Load the dataset\n",
    "train_dataset = LazyImageDataset(\n",
    "    image_dir = \"../aocr2024/preprocessed_images/\",\n",
    "    labels_file = \"../aocr2024/TrainValid_ground_truth.csv\",\n",
    "    validation_file = \"../aocr2024/TrainValid_split.csv\",\n",
    "    split = \"Train\",\n",
    ")\n",
    "val_dataset = LazyImageDataset(\n",
    "    image_dir = \"../aocr2024/preprocessed_images/\",\n",
    "    labels_file = \"../aocr2024/TrainValid_ground_truth.csv\",\n",
    "    validation_file = \"../aocr2024/TrainValid_split.csv\",\n",
    "    split = \"Valid\",\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception [Errno 2] No such file or directory: '../tmp/tmp_execution_8.pth'\n",
      "Continue training after 0 for epochs 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/50 [08:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mfloat(), labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/Github/AOCR2024_pvt/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/AOCR2024_pvt/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model\n",
    "if selected_model == \"r3d18\":\n",
    "    model = ModifiedR3D18().to(device)\n",
    "elif selected_model == \"mc3d18\":\n",
    "    model = ModifiedMC3_18().to(device)\n",
    "else:\n",
    "    raise RuntimeError\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# Move the model if possible\n",
    "if device == torch.device(\"mps\"):\n",
    "    model.to_device(device)\n",
    "    print(f\"Model and weights moved to GPU (MPS Mac)\")\n",
    "model.train()\n",
    "# Load model if exists\n",
    "try:\n",
    "    file_name = \"../tmp/tmp_execution_8.pth\"\n",
    "    model.load_state_dict(torch.load(file_name))\n",
    "    original_epochs = file_name.split(\".\")[2].split(\"_\")[-1]\n",
    "    print(f\"Loaded the model from {file_name}\")\n",
    "except Exception as e:\n",
    "    original_epochs = 0\n",
    "    print(f\"Exception {e}\")\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 30\n",
    "print(f\"Continue training after {original_epochs} for epochs {num_epochs}\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch_idx, (images, labels) in progress_bar:\n",
    "        images, labels = images.to(device), labels.reshape((labels.size(0),1)).to(device)\n",
    "        start = time.time()\n",
    "        outputs = model(images)\n",
    "        end = time.time()\n",
    "        loss = criterion(outputs.float(), labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # Accuracy\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predicted = (probabilities > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        average_accuracy = correct_predictions / total_predictions\n",
    "        # Memory\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                'loss': total_loss / (batch_idx + 1), \n",
    "                'accuracy': average_accuracy,\n",
    "                'memory_used': f\"{memory_info.used / (1024**2):2f}\",\n",
    "                'memory_availabl': f\"{memory_info.available / (1024**2):.2f}MB\",\n",
    "                'prediction_time': f\"{end - start}s\"\n",
    "            }\n",
    "        )\n",
    "    # Validation\n",
    "    valid_correct_predictions = 0\n",
    "    valid_total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            labels = labels.reshape((labels.size(0),1))\n",
    "            outputs = model(images)\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predicted = (probabilities > 0.5).float()\n",
    "            valid_correct_predictions += (predicted == labels).sum().item()\n",
    "            valid_total_predictions += labels.size(0)\n",
    "        average_accuracy = valid_correct_predictions / valid_total_predictions\n",
    "        print(f\"Validation accuracy: {average_accuracy * 100:.2f}% Validation items: {valid_total_predictions}\")\n",
    "    # Store the current model just in case of failure\n",
    "    torch.save(model.state_dict(), f'../tmp/tmp_execution_{int(epoch) + int(original_epochs) + 1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../model_store/execution_{int(num_epochs) + int(original_epochs)}.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModifiedR3D18()\n",
    "model.train()\n",
    "# Load model if exists\n",
    "try:\n",
    "    file_name = \"../model_store/execution_20.pth\"\n",
    "    model.load_state_dict(torch.load(file_name))\n",
    "    original_epochs = file_name.split(\".\")[2].split(\"_\")[-1]\n",
    "    print(f\"Loaded the model from {file_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"First train a model you lazy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_dataset = LazyImageDataset(\n",
    "    image_dir = \"../aocr2024/preprocessed_images_test/\",\n",
    "    labels_file = \"../aocr2024/TrainValid_ground_truth.csv\",\n",
    "    validation_file = \"../aocr2024/TrainValid_split.csv\",\n",
    "    split = \"Test\",\n",
    ")\n",
    "# Test loader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "ids = []\n",
    "for images, labels in test_loader:\n",
    "    outputs = model(images)\n",
    "    probabilities = torch.sigmoid(outputs)\n",
    "    predicted = list((probabilities > 0.5).float())\n",
    "    classifications += predicted\n",
    "    ids += list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission csv\n",
    "submission = pd.DataFrame(\n",
    "    zip(ids, classifications),\n",
    "    columns = [\n",
    "        \"id\",\n",
    "        \"label\"\n",
    "    ]\n",
    ")\n",
    "submission[\"label\"] = submission.label.astype(int)\n",
    "submission.to_csv(\"../aocr2024/submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
